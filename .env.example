# Local Llama 3.1 Configuration
# The app will automatically try these endpoints in order:

# Ollama (most common)
VITE_LLAMA_OLLAMA_URL=http://localhost:11434/api/generate

# llama.cpp server (OpenAI-compatible)
VITE_LLAMA_CPP_URL=http://localhost:8080/v1/chat/completions

# Custom endpoint
VITE_LLAMA_CUSTOM_URL=http://localhost:5000/api/generate

# Model Configuration
VITE_LLAMA_MODEL=llama3.1
VITE_LLAMA_TEMPERATURE=0.3
VITE_LLAMA_MAX_TOKENS=500

# Fallback to cloud APIs (optional)
# VITE_OPENAI_API_KEY=your_openai_api_key_here
# VITE_ANTHROPIC_API_KEY=your_anthropic_api_key_here
